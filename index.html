<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Ola, Omni-Modal Language Model">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Ola</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://yushi-hu.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
            Fine-Grained RLHF (NeurIPS 2023)
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-five-sixths">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title"><span style="color: rgb(247,178,106);">Ola</span>: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment</h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
              <span class="author-block">
                <a href='https://github.com/liuzuyan' target="_blank"><font color="#B082C9"><b>Zuyan Liu</b></font></a><sup>1,2,*</sup>&emsp;
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=zh-CN&user=kMui170AAAAJ" target="_blank"><font color="#B082C9"><b>Yuhao Dong</b></font></a><sup>2,3,*</sup>&emsp;
              </span>
              <span class="author-block">
                <a href="" target="_blank"><font color="#B082C9"><b>Jiahui Wang</b></font></a><sup>1</sup>&emsp;
              </span>
              <br>
              <span class="author-block">
                <a href="https://liuziwei7.github.io/" target="_blank"><font color="#B082C9"><b>Ziwei Liu</b></font></a><sup>3</sup>&emsp;
              </span>
              <span class="author-block">
                <font color="#B082C9"><b>Winston Hu</b></font></a><sup>2</sup>&emsp;
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=TN8uDQoAAAAJ&hl=en" target="_blank"><font color="#B082C9"><b>Jiwen Lu</b></font></a><sup>1</sup>&emsp;
              </span>
              <span class="author-block">
                <a href="https://raoyongming.github.io/" target="_blank"><font color="#B082C9"><b>Yongming Rao</b></font></a><sup>2,1</sup>&emsp;
              </span>
              </div>

                <div class="is-size-5 publication-authors">
                  <span class="author-block">
                    <sup>1</sup>Tsinghua University&emsp;
                    <sup>2</sup>Tencent&emsp;
                    <sup>3</sup>S-Lab, NTU&emsp;
                    <sup>*</sup>Equal Contribution&emsp;
                  </span>
                  <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
                  <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span> -->
                </div>

                <div class="content has-text-centered">
                  <div class="publication-links">
                    <span class="link-block">
                      <a href="https://arxiv.org/abs/2501.xxxx" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://huggingface.co/THUdyh/Ola-7b" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-database"></i>
                    </span>
                    <span>Checkpoint</span>
                  </a>
                </span>
                </span>

              <span class="link-block">
                <a href="https://huggingface.co/spaces/THUdyh/xxxx" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span>üé®Demo(Coming Soon)</span>
              </a>
            </span>
            </span>

                <span class="link-block">
                  <a href="https://github.com/Ola-Omni/Ola" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
                </span>

              <!-- <span class="link-block">
                <a href="https://x.com/_akhaliq/status/1836963718887866400" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-twitter"></i>
                </span>
                <span>Twitter</span>
                </a>
              </span> -->
            </div>
        </div>
      </div>
    </div>
  </div>
</div>
</section>

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance.
            In this paper, we present <b>Ola</b>, an <b>O</b>mni-modal <b>la</b>nguage model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts.  
            The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly.  Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation.
            Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel examples-->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Examples of Ola</h2> <br></div>
  </div></section>

<section class="hero is-small">
  <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <!-- Your image here -->
              <img src="static/images/case1.png" width="100%"/>
            </div>

           <div class="item">
            <img src="static/images/case2.png" width="150%"/>
          </div>
    
          <div class="item">
          <img src="static/images/case3.png" width="150%"/>
          </div>
    
          <div class="item">
          <img src="static/images/case4.png" height="100%"/>
          </div>
    
          </div>
        </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">Ola Performances</h2>
      <br>
      <b>Ola pushes the frontiers of the omni-modal language model across image, video and audio understanding benchmarks.</b> We
compare Ola with existing state-of-the-art open-sourced multimodal models and GPT-4o on their abilities in mainstream image, video, and
audio benchmarks. For fair comparisons, we select around 7B versions of existing MLLMs. Ola can achieve outperforming performance
against omni-modal and specialized MLLMs in all modalities thanks to our progressive alignment strategy. ‚Äú√ó‚Äù indicates that the model is
not capable of the task and ‚Äú‚àí‚Äù indicates the result is lacking. The score for LibriSpeech is inverted as lower is better for the WER metric.
      <img src="static/images/teaser.png" height="100%"/>
      <!-- <h2 class="subtitle has-text-centered">Example tasks in <span style="font-weight:bold;">BLINK</span>.</h2> -->
      
    </div>
  </div>
</section>
<!-- End teaser image -->

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Ola Architecture</h2>
        <h2 class="content has-text-justified">
          <b>Ola Architecture.</b> Ola supports omni-modal inputs including text, image, video, and audio, capable of processing the inputs
          simultaneously with competitive performance on understanding tasks for all these modalities. Meanwhile, Ola supports user-friendly
          real-time streaming decoding for texts and speeches thanks to the text detokenizer and the speech decoder.
        </h2>
        <img src="static/images/method.png" height="100%"/>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Ola Training Strategies</h2>
        <h2 class="content has-text-justified">
          <b>Illustrations of the Ola Progressive Modality Alignment.</b> We visualize the relationships among modalities in the left part.
          Speech acts as the connection between language and audio knowledge, while video constructs the bridge with highly relevant visual and
          audio information. Therefore, we design the progressive alignment training strategy from primary to periphery. Furthermore, we design the
          cross-modality video-audio data to better capture the relationships among modalities.
        </h2>
        <img src="static/images/training.png" height="100%"/>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Benchmark Performance</h2>
        <h2 class="content has-text-justified">
          <b>Main Results across Image, Video, and Audio Understanding Benchmarks.</b> We select representative benchmarks among image,
          video, and audio benchmarks, and select mainstream state-of-the-art open-source large language models in each modality. We also include
          open-source omni-modal LLMs for comparison. In the table, ‚Äù‚àí‚Äù indicates the model is capable of solving the tasks theoretically, while
          the result is lacking. ‚Äù‚úó‚Äù indicates that the model is not capable of the task. ‚Üì indicates that lower score is better. * LLaMA-Omni is not
          optimized for ASR and thus cannot produce reasonable results on this task.
        </h2>
        <img src="static/images/results.png" height="100%"/>
      </div>
    </div>
  </div>
</section>


<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation (BibTeX)</h2><pre><code>
    
    </code></pre>
  </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
<div class="container">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">

        <p>
          This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
          
        </p>

      </div>
    </div>
  </div>
</div>
</footer>
  </body>
  </html>
